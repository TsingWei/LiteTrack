# Copyright (c) ByteDance, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
Mostly copy-paste from DINO and timm library:
https://github.com/facebookresearch/dino
https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
"""

import imp
import os
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from .utils import combine_tokens, recover_tokens
from functools import partial
from timm.models.layers import to_2tuple


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    # if (mean < a - 2 * std) or (mean > b + 2 * std):
    #     warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
    #                   "The distribution of values may be incorrect.",
    #                   stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=None):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        #self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        ####add by wxd
        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        all_head_dim = head_dim * self.num_heads
        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))
            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))
        else:
            self.q_bias = None
            self.v_bias = None
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, len_t=None, x_rel_pos_bias = None, return_attention = True, mask=None):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        if x_rel_pos_bias is not None:
            attn = attn + x_rel_pos_bias.unsqueeze(0)
        if mask is not None:
            attn = attn.masked_fill(~mask, float('-inf'),)
        attn_nosf = attn
        attn = attn.softmax(dim=-1)
        
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        if return_attention:
            if mask is not None:
                attn = attn[:,:,:-1,:-1]
            return x, attn_nosf
        else:
            return x

class CenterAttention(Attention):
    # def __init__(self, **kwargs):
    #     super().__init__(**kwargs)
    #     self.center_attn_mask = 
    def forward(self, x, len_t=None, x_rel_pos_bias=None, return_attention=True, mask=None):
        
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        if x_rel_pos_bias is not None:
            attn = attn + x_rel_pos_bias.unsqueeze(0)
        if mask is not None:
            attn = attn.masked_fill(~mask, float('-inf'),)
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        if return_attention:
            return x, attn
        else:
            return x

class SingleCrossAttention(Attention):
    def forward(self, x, len_t=None, x_rel_pos_bias = None, return_attention = True):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        q_t, k_t, v_t = q[:, :, :len_t], k[:, :, :len_t], v[:, :, :len_t]
        q_s, k_s, v_s = q[:, :, len_t:], k[:, :, len_t:], v[:, :, len_t:]

        attn_t2t = (q_t @ k_t.transpose(-2, -1)) * self.scale
        attn_s2t = (q_s @ k_t.transpose(-2, -1)) * self.scale

        attn_t2t = attn_t2t.softmax(dim=-1)
        attn_s2t = attn_s2t.softmax(dim=-1)

        attn_t2t = self.attn_drop(attn_t2t)
        attn_s2t = self.attn_drop(attn_s2t)

        z = (attn_t2t @ v_t).transpose(1, 2).reshape(B, len_t, C)
        x = (attn_s2t @ v_t).transpose(1, 2).reshape(B, N-len_t, C)
        z = self.proj(z)
        x = self.proj(x)
        z = self.proj_drop(z)
        x = self.proj_drop(x)
        if return_attention:
            return torch.cat((z, x), dim=1), None
        else:
            return torch.cat((z, x), dim=1)

class CrossAttention(Attention):
    def forward(self, x, len_t=None, x_rel_pos_bias = None, return_attention = True):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        q_t, k_t, v_t = q[:, :, :len_t], k[:, :, :len_t], v[:, :, :len_t]
        q_s, k_s, v_s = q[:, :, len_t:], k[:, :, len_t:], v[:, :, len_t:]

        attn_t2s = (q_t @ k_s.transpose(-2, -1)) * self.scale
        # attn_s2t = (q_s @ k_t.transpose(-2, -1)) * self.scale
        attn_s2t = attn_t2s.transpose(-1,-2)
        if x_rel_pos_bias is not None:
            # TODO Should be seprate RPB.
            attn_t2t = attn_t2t + x_rel_pos_bias.unsqueeze(0)
            attn_s2s = attn_s2s + x_rel_pos_bias.unsqueeze(0)

        attn_t2s = attn_t2s.softmax(dim=-1)
        attn_s2t = attn_s2t.softmax(dim=-1)
        attn_t2s = self.attn_drop(attn_t2s)
        attn_s2t = self.attn_drop(attn_s2t)

        z = (attn_t2s @ v_s).transpose(1, 2).reshape(B, len_t, C)
        x = (attn_s2t @ v_t).transpose(1, 2).reshape(B, N-len_t, C)
        z = self.proj(z)
        x = self.proj(x)
        z = self.proj_drop(z)
        x = self.proj_drop(x)
        if return_attention:
            return torch.cat((z, x), dim=1), None
        else:
            return torch.cat((z, x), dim=1)

class MixedCrossAttention(Attention):
    def forward(self, x, len_t, x_rel_pos_bias = None, return_attention = True):
        # z = x[:,:len_t]
        # x = x[:,len_t:]
        # len_t = len_t+1 # attend cls token
        # FIX: add cls outside
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        q_s = q[:, :, len_t:]

        attn = (q_s @ k.transpose(-2, -1)) * self.scale
        if x_rel_pos_bias is not None:
            attn = attn + x_rel_pos_bias.unsqueeze(0)

        attn = self.attn_drop(attn)
        # attn_t2t = attn[:, :, :len_t, :len_t]   # BS, Nh, len_t, len_t
        # attn_s2ts = attn[:, :, len_t:, :]       # BS, Nh, len_t+s, len_s
        attn_s2ts = attn

        # attn_t2t = attn_t2t.softmax(dim=-1)
        attn_s2ts = attn_s2ts.softmax(dim=-1)

        # x_t = (attn_t2t @ v[:, :, :len_t]).transpose(1, 2).reshape(B, len_t, C)
        x_t = x[:, :len_t]
        x_s = (attn_s2ts @ v).transpose(1, 2).reshape(B, N-len_t, C)
        x = torch.cat((x_t, x_s), dim=1)
        # x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        if return_attention:
            return x, attn
        else:
            return x

class HybridCrossAttention(Attention):
    # x: Concated tokens of:
    #       1. {POOLED tokens} from Template.
    #       2. {tokens} from Search.
    # Return: Search part of x.
    def forward(self, x, len_t=None, x_rel_pos_bias = None, return_attention = True, template_bb=None):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        # q_t, k_t, v_t = q[:, :, :len_t], k[:, :, :len_t], v[:, :, :len_t]
        q_s, k_s, v_s = q[:, :, len_t:], k[:, :, len_t:], v[:, :, len_t:]

        # attn_t2s = (q_t @ k_s.transpose(-2, -1)) * self.scale
        attn_s2ts = (q_s @ k.transpose(-2, -1)) * self.scale
        if x_rel_pos_bias is not None:
            # TODO Should be seprate RPB.
            attn_t2t = attn_t2t + x_rel_pos_bias.unsqueeze(0)
            attn_s2s = attn_s2s + x_rel_pos_bias.unsqueeze(0)

        # attn_t2s = attn_t2s.softmax(dim=-1)
        attn_s2ts = attn_s2ts.softmax(dim=-1)
        # attn_t2s = self.attn_drop(attn_t2s)
        attn_s2ts = self.attn_drop(attn_s2ts)

        # z = (attn_t2s @ v_s).transpose(1, 2).reshape(B, len_t, C)
        x = (attn_s2ts @ v).transpose(1, 2).reshape(B, N-len_t, C)
        # z = self.proj(z)
        x = self.proj(x)
        # z = self.proj_drop(z)
        x = self.proj_drop(x)
        if return_attention:
            # return torch.cat((z, x), dim=1), None
            return x, None
        else:
            return x


class SeperateSelfAttention(Attention):
    def forward(self, x, len_t=None, x_rel_pos_bias = None, return_attention = True):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4) # 3, B, nH, N, dim
        q, k, v = qkv[0], qkv[1], qkv[2]

        q_t, k_t, v_t = q[:, :, :len_t], k[:, :, :len_t], v[:, :, :len_t]
        q_s, k_s, v_s = q[:, :, len_t:], k[:, :, len_t:], v[:, :, len_t:]

        attn_t2t = (q_t @ k_t.transpose(-2, -1)) * self.scale
        attn_s2s = (q_s @ k_s.transpose(-2, -1)) * self.scale
        if x_rel_pos_bias is not None:
            # TODO Should be seprate RPB.
            attn_t2t = attn_t2t + x_rel_pos_bias.unsqueeze(0)
            attn_s2s = attn_s2s + x_rel_pos_bias.unsqueeze(0)

        # attn_s2t = (q_s @ k_t.transpose(-2, -1)) * self.scale
        # attn_s2t = attn_s2t.softmax(dim=-1)
        # attn_s2t = self.attn_drop(attn_s2t)

        attn_t2t = attn_t2t.softmax(dim=-1)
        attn_s2s = attn_s2s.softmax(dim=-1)
        attn_t2t = self.attn_drop(attn_t2t)
        attn_s2s = self.attn_drop(attn_s2s)

        z = (attn_t2t @ v_t).transpose(1, 2).reshape(B, len_t, C)
        x = (attn_s2s @ v_s).transpose(1, 2).reshape(B, N-len_t, C)
        z = self.proj(z)
        x = self.proj(x)
        z = self.proj_drop(z)
        x = self.proj_drop(x)
        if return_attention:
            return torch.cat((z, x), dim=1), None
        else:
            return x

class CrossAttention_CAE(nn.Module):
    def __init__(
            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,
            proj_drop=0., window_size=None, attn_head_dim=None):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        if attn_head_dim is not None:
            head_dim = attn_head_dim
        all_head_dim = head_dim * self.num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.q = nn.Linear(dim, all_head_dim, bias=False)
        self.k = nn.Linear(dim, all_head_dim, bias=False)
        self.v = nn.Linear(dim, all_head_dim, bias=False)

        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))
            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))
        else:
            self.q_bias = None
            self.k_bias = None
            self.v_bias = None

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(all_head_dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, bool_masked_pos=None, k=None, v=None):
        B, N, C = x.shape
        N_k = k.shape[1]
        N_v = v.shape[1]

        q_bias, k_bias, v_bias = None, None, None
        if self.q_bias is not None:
            q_bias = self.q_bias
            k_bias = torch.zeros_like(self.v_bias, requires_grad=False)
            v_bias = self.v_bias

        q = F.linear(input=x, weight=self.q.weight, bias=q_bias)
        q = q.reshape(B, N, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)    # (B, N_head, N_q, dim)

        k = F.linear(input=k, weight=self.k.weight, bias=k_bias)
        k = k.reshape(B, N_k, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)

        v = F.linear(input=v, weight=self.v.weight, bias=v_bias)   
        v = v.reshape(B, N_v, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))      # (B, N_head, N_q, N_k)
        
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, -1) 
        x = self.proj(x)
        x = self.proj_drop(x)

        return x

class AttentiveBlock(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 window_size=None, attn_head_dim=None):
        super().__init__()

        self.norm1_q = norm_layer(dim)
        self.norm1_k = norm_layer(dim)
        self.norm1_v = norm_layer(dim)
        self.norm2_cross = norm_layer(dim)
        self.cross_attn =  CrossAttention_CAE(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        
    def forward(self, x_q, x_kv, pos_q, pos_k, bool_masked_pos, rel_pos_bias=None):
        x_q = self.norm1_q(x_q + pos_q)
        x_k = self.norm1_k(x_kv + pos_k)
        x_v = self.norm1_v(x_kv)

        x = self.cross_attn(x_q, k=x_k, v=x_v)

        return x

class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., seperate_self_attn=False,
                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, window_size=None, init_values=0,
                 add_cls_token=False):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.add_cls_token = add_cls_token
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
        self.seperate_self_attn = seperate_self_attn
        if not self.seperate_self_attn:
            self.attn = MixedCrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, 
                    attn_drop=attn_drop, proj_drop=drop, window_size=window_size)
        else:
            self.attn = HybridCrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, 
                    attn_drop=attn_drop, proj_drop=drop, window_size=window_size)
        if init_values > 0:
            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)
            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)
        else:
            self.gamma_1, self.gamma_2 = None, None

    def forward(self, x, len_t, x_rel_pos_bias=None, return_attention=False, mask=None):
        # if self.seperate_self_attn:
        #     t = x[:, :len_t]
        #     s = x[:, len_t:]
        #     t_out, t_attn = self.attn(self.norm1(t), len_t, x_rel_pos_bias) # If ori Attn, 'len_t' is not used.
        #     s_out, s_attn = self.attn(self.norm1(s), len_t, x_rel_pos_bias)
        #     y = torch.cat((t_out, s_out), dim=1)
        #     attn = s_attn
        # else:
        z_in = x[:, :len_t]
        x_in = x[:, len_t:]
        # in_attn = torch.cat((z_in, self.norm1(x[:, len_t:])), dim=1)
        
        y, attn = self.attn(self.norm1(x), len_t, x_rel_pos_bias)
        y_out = y[:, len_t:]
        # x = x[:, len_t:]
        if return_attention:
            return attn
        if self.gamma_1 is None:
            x = x + self.drop_path(y)
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        else:
            x_in = x_in + self.drop_path(self.gamma_1 * y_out)
            x_in = x_in + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_in)))
        out = torch.cat((z_in, x_in), dim=1)
        return out, attn

class CAE_CEBlock(Block):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., keep_ratio_search=1.0,
                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, window_size=None, init_values=0):
        super().__init__(dim = dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop,
        attn_drop=attn_drop, drop_path=drop_path, act_layer=act_layer, norm_layer=norm_layer, window_size=window_size, init_values=init_values)
        self.keep_ratio_search = keep_ratio_search
    def forward(self, x, global_index_template, global_index_search, x_rel_pos_bias=None, mask=None, ce_template_mask=None, keep_ratio_search=None, use_target_token=False):
        lens_t = global_index_template.shape[1]
        lens_all = x.shape[1]
        # if use_target_token:
        #     mask = generate_mask_for_target_token(mask, lens_all, lens_t)
        # else:
        #     mask = None
        y, attn = self.attn(self.norm1(x), x_rel_pos_bias)
        if self.gamma_1 is None:
            x = x + self.drop_path(y)
        else:
            x = x + self.drop_path(self.gamma_1 * y)
        # if use_target_token:
        #     target_token = x[:,-1:]
        #     x = x[:,:-1]

        removed_index_search = None
        if self.keep_ratio_search < 1 and (keep_ratio_search is None or keep_ratio_search < 1):
            keep_ratio_search = self.keep_ratio_search if keep_ratio_search is None else keep_ratio_search
            x, global_index_search, removed_index_search = candidate_elimination(attn, x, lens_t, keep_ratio_search, global_index_search, ce_template_mask)
        # x = x + self.drop_path(self.mlp(self.norm2(x)))
        if self.gamma_1 is None:
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        else:
            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
        # if use_target_token:
        #     x = torch.cat([x, target_token], dim=1)
        return x, global_index_template, global_index_search, removed_index_search, attn

def candidate_elimination(attn: torch.Tensor, tokens: torch.Tensor, lens_t: int, keep_ratio: float, global_index: torch.Tensor, box_mask_z: torch.Tensor):
    """
    Eliminate potential background candidates for computation reduction and noise cancellation.
    Args:
        attn (torch.Tensor): [B, num_heads, L_t + L_s, L_t + L_s], attention weights
        tokens (torch.Tensor):  [B, L_t + L_s, C], template and search region tokens
        lens_t (int): length of template
        keep_ratio (float): keep ratio of search region tokens (candidates)
        global_index (torch.Tensor): global index of search region tokens
        box_mask_z (torch.Tensor): template mask used to accumulate attention weights

    Returns:
        tokens_new (torch.Tensor): tokens after candidate elimination
        keep_index (torch.Tensor): indices of kept search region tokens
        removed_index (torch.Tensor): indices of removed search region tokens
    """
    lens_s = attn.shape[-1] - lens_t
    bs, hn, _, _ = attn.shape

    lens_keep = math.ceil(keep_ratio * lens_s)
    if lens_keep == lens_s:
        return tokens, global_index, None

    attn_t = attn[:, :, :lens_t, lens_t:]

    if box_mask_z is not None:
        box_mask_z = box_mask_z.unsqueeze(1).unsqueeze(-1).expand(-1, attn_t.shape[1], -1, attn_t.shape[-1])
        # attn_t = attn_t[:, :, box_mask_z, :]
        attn_t = attn_t[box_mask_z]
        attn_t = attn_t.view(bs, hn, -1, lens_s)
        attn_t = attn_t.mean(dim=2).mean(dim=1)  # B, H, L-T, L_s --> B, L_s

        # attn_t = [attn_t[i, :, box_mask_z[i, :], :] for i in range(attn_t.size(0))]
        # attn_t = [attn_t[i].mean(dim=1).mean(dim=0) for i in range(len(attn_t))]
        # attn_t = torch.stack(attn_t, dim=0)
    else:
        attn_t = attn_t.mean(dim=2).mean(dim=1)  # B, H, L-T, L_s --> B, L_s

    # use sort instead of topk, due to the speed issue
    # https://github.com/pytorch/pytorch/issues/22812
    sorted_attn, indices = torch.sort(attn_t, dim=1, descending=True)

    topk_attn, topk_idx = sorted_attn[:, :lens_keep], indices[:, :lens_keep]
    non_topk_attn, non_topk_idx = sorted_attn[:, lens_keep:], indices[:, lens_keep:]

    keep_index = global_index.gather(dim=1, index=topk_idx)
    removed_index = global_index.gather(dim=1, index=non_topk_idx)

    # separate template and search tokens
    tokens_t = tokens[:, :lens_t]
    tokens_s = tokens[:, lens_t:]

    # obtain the attentive and inattentive tokens
    B, L, C = tokens_s.shape
    # topk_idx_ = topk_idx.unsqueeze(-1).expand(B, lens_keep, C)
    attentive_tokens = tokens_s.gather(dim=1, index=topk_idx.unsqueeze(-1).expand(B, -1, C))
    # inattentive_tokens = tokens_s.gather(dim=1, index=non_topk_idx.unsqueeze(-1).expand(B, -1, C))

    # compute the weighted combination of inattentive tokens
    # fused_token = non_topk_attn @ inattentive_tokens

    # concatenate these tokens
    # tokens_new = torch.cat([tokens_t, attentive_tokens, fused_token], dim=0)
    tokens_new = torch.cat([tokens_t, attentive_tokens], dim=1)

    return tokens_new, keep_index, removed_index

def generate_mask_for_target_token(center_attn_mask, len_all, len_t):

    # len_inner = len_t + 1
    center_attn_mask = center_attn_mask[:len_all,:len_all]
    center_attn_mask[:,:] = True
    # center_attn_mask = torch.ones([len_all, len_all], dtype=torch.bool)
    # center_attn_mask[:len_inner, :len_inner] = True # Self attn in Template and TargetToken
    # center_attn_mask[len_inner:, len_inner:] = True # Self attn in Search
    center_attn_mask[len_t:-1, -1:] = False #  
    center_attn_mask[-1:, len_t:-1] = False #
    return center_attn_mask.detach()

class CEBlock(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, keep_ratio_search=1.0,):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        self.keep_ratio_search = keep_ratio_search

    def forward(self, x, global_index_template, global_index_search, mask=None, ce_template_mask=None, keep_ratio_search=None):
        x_attn, attn = self.attn(self.norm1(x), mask, True)
        x = x + self.drop_path(x_attn)
        lens_t = global_index_template.shape[1]

        removed_index_search = None
        if self.keep_ratio_search < 1 and (keep_ratio_search is None or keep_ratio_search < 1):
            keep_ratio_search = self.keep_ratio_search if keep_ratio_search is None else keep_ratio_search
            x, global_index_search, removed_index_search = candidate_elimination(attn, x, lens_t, keep_ratio_search, global_index_search, ce_template_mask)

        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x, global_index_template, global_index_search, removed_index_search, attn

class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """
    def __init__(self, img_size=[224, 224], patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.num_patches_w = img_size[0] // patch_size
        self.num_patches_h = img_size[1] // patch_size

        num_patches = self.num_patches_w * self.num_patches_h
        self.patch_shape = (img_size[0] // patch_size, img_size[1] // patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        # print("##############patch here!!!")
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.flatten = True
            
    def forward(self, x, mask=None):
        # B, C, H, W = x.shape
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
        # x = self.norm(x)
        return x


class VIT_Backbone(nn.Module):
    def __init__(self,
                 img_size=[224,224],
                 patch_size=16,
                 embed_dim=768,
                 in_chans=3,
                 frozen_stages=0,
                #  out_indices=[3, 5, 7, 11],
                 out_with_norm=False,
                 use_checkpoint=False,
                 CE=False,
                 add_cls_token = False,
                 add_template_target_embed=False,
                 num_classes=0,  
                 depth=12,
                 num_heads=12, 
                 mlp_ratio=4., 
                 qkv_bias=False, 
                 qk_scale=None, 
                 drop_rate=0., 
                 attn_drop_rate=0.,
                 drop_path_rate=0., 
                 norm_layer=partial(nn.LayerNorm, eps=1e-6), 
                 return_all_tokens=False, 
                 init_values=0, 
                 use_sincos_pos_emb=False, 
                 use_abs_pos_emb=False, 
                 use_rel_pos_bias=False, 
                 seperate_loc=None,
                 act_layer=None, 
                 ce_loc=None, 
                 ce_keep_ratio=None, 
                 use_mean_pooling=False, 
                 masked_im_modeling=False,
                 ):
        
        super().__init__()

        self.num_features = self.embed_dim = embed_dim
        self.return_all_tokens = return_all_tokens
        # print("############use_abs_pos:", use_abs_pos_emb)
        # print("############use_sincos_pos:", use_sincos_pos_emb)
        # print("############use_rel_pos_bias:", use_rel_pos_bias)
        self.use_abs_pos_emb = use_abs_pos_emb
        self.use_sincos_pos_emb = use_sincos_pos_emb
        self.CE = CE
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        if use_abs_pos_emb:
            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        else:
            self.pos_embed = None

        self.pos_drop = nn.Dropout(p=drop_rate)
        act_layer = act_layer or nn.GELU
        self.use_rel_pos_bias = use_rel_pos_bias

        if self.use_rel_pos_bias:
            print("=================use RelativePositionBias===================")
            window_size=self.patch_embed.patch_shape
            self.window_size = window_size
            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3
            self.relative_position_bias_table = nn.Parameter(
                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH
            # cls to token & token 2 cls & cls to cls

        else:
            self.window_size = None
            self.relative_position_bias_table = None
            self.relative_position_index = None

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        if self.CE:    
            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
            blocks = []
            ce_index = 0
            self.ce_loc = ce_loc
            for i in range(depth):
                ce_keep_ratio_i = 1.0
                if ce_loc is not None and i in ce_loc:
                    ce_keep_ratio_i = ce_keep_ratio[ce_index]
                    ce_index += 1

                blocks.append(
                    CAE_CEBlock(
                        dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, 
                        init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,
                        keep_ratio_search=ce_keep_ratio_i)
                )
            self.blocks = nn.ModuleList(blocks)
        else:
            blocks = []
            sep_index = 0
            self.seperate_loc = seperate_loc
            for i in range(depth):
                sep_i = False
                if seperate_loc is not None and i in seperate_loc:
                    sep_i = True
                    sep_index += 1
                blocks.append(Block(
                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, seperate_self_attn = sep_i,
                    init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None))
            self.blocks = nn.ModuleList(blocks)

        # self.attentive_blocks_ = nn.ModuleList([
        #             AttentiveBlock(
        #                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
        #                 drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, norm_layer=norm_layer,
        #                 init_values=0)
        #             for i in range(3)])    

        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)
        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None
        # Classifier head
        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        
        if use_abs_pos_emb:
            trunc_normal_(self.pos_embed, std=.02)
        trunc_normal_(self.cls_token, std=.02)
        self.apply(self._init_weights)

        # masked image modeling
        self.masked_im_modeling = masked_im_modeling
        if masked_im_modeling:
            self.masked_embed = nn.Parameter(torch.zeros(1, embed_dim))

        # support non-square image as input
        if len(img_size) == 1:
            img_size = img_size * 2
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches
        if self.use_abs_pos_emb:
            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        elif self.use_sincos_pos_emb:
            self.pos_embed = self.build_2d_sincos_position_embedding(embed_dim)
        else:
            self.pos_embed = None

        self.add_cls_token = add_cls_token
        self.patch_size = patch_size
        # self.with_fpn = with_fpn
        self.frozen_stages = frozen_stages
        # self.out_indices = out_indices
        self.use_checkpoint = use_checkpoint
        self.cat_mode = 'direct'
        self.add_template_target_embed = add_template_target_embed

        self.pos_embed_z = None
        self.pos_embed_x = None
        # if self.add_cls_token:
        #     # assert self.CE == False # Not implement yet for use both.
        #     # self.cls_embed = Mlp(4, out_features=self.embed_dim)

        if not out_with_norm:
            self.norm = nn.Identity()
        # self.prpool = PrRoIPool([2,2])

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def prepare_tokens(self, x, mask=None):
        B, nc, w, h = x.shape
        # patch linear embedding
        x = self.patch_embed(x)

        # mask image modeling
        if mask is not None:
            x = self.mask_model(x, mask)
        x = x.flatten(2).transpose(1, 2)

        # add the [CLS] token to the embed patch tokens
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        
        # add positional encoding to each token
        if self.pos_embed is not None:
            x = x + self.interpolate_pos_encoding(x, w, h)

        return self.pos_drop(x)

    def get_last_selfattention(self, x):
        x = self.prepare_tokens(x)
        for i, blk in enumerate(self.blocks):
            if i < len(self.blocks) - 1:
                x = blk(x)
            else:
                # return attention of the last block
                return blk(x, return_attention=True)

    def get_intermediate_layers(self, x, n=1):
        x = self.prepare_tokens(x)
        # we return the output tokens from the `n` last blocks
        output = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if len(self.blocks) - i <= n:
                output.append(self.norm(x))
        return output
        
    def get_num_layers(self):
        return len(self.blocks)

    def build_2d_sincos_position_embedding(self, embed_dim=768, temperature=10000., decode=False):
        h, w = self.patch_embed.patch_shape 
        grid_w = torch.arange(w, dtype=torch.float32)
        grid_h = torch.arange(h, dtype=torch.float32)
        grid_w, grid_h = torch.meshgrid(grid_w, grid_h)
        assert embed_dim % 4 == 0, 'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'
        pos_dim = embed_dim // 4
        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim
        omega = 1. / (temperature ** omega)
        out_w = torch.einsum('m,d->md', [grid_w.flatten(), omega])
        out_h = torch.einsum('m,d->md', [grid_h.flatten(), omega])
        pos_emb = torch.cat([torch.sin(out_w), torch.cos(out_w), torch.sin(out_h), torch.cos(out_h)], dim=1)[None, :, :]

        pe_token = torch.zeros([1, 1, embed_dim], dtype=torch.float32)
        pos_embed = nn.Parameter(torch.cat([pe_token, pos_emb], dim=1))
        pos_embed.requires_grad = False
        return pos_embed
    
    def finetune_track(self, cfg, patch_start_index=1):
        # patch_start_index: For skipping the cls embedding
        search_size = to_2tuple(cfg.DATA.SEARCH.SIZE)
        template_size = to_2tuple(cfg.DATA.TEMPLATE.SIZE)
        new_patch_size = cfg.MODEL.BACKBONE.STRIDE

        self.cat_mode = cfg.MODEL.BACKBONE.CAT_MODE
        self.return_inter = cfg.MODEL.RETURN_INTER
        self.add_sep_seg = cfg.MODEL.BACKBONE.SEP_SEG

        # resize patch embedding
        if new_patch_size != self.patch_size:
            print('Inconsistent Patch Size With The Pretrained Weights, Interpolate The Weight!')
            old_patch_embed = {}
            for name, param in self.patch_embed.named_parameters():
                if 'weight' in name:
                    param = nn.functional.interpolate(param, size=(new_patch_size, new_patch_size),
                                                      mode='bicubic', align_corners=False)
                    param = nn.Parameter(param)
                old_patch_embed[name] = param
            # self.patch_embed = PatchEmbed(img_size=self.img_size, patch_size=new_patch_size, in_chans=3,
            #                               embed_dim=self.embed_dim)
            self.patch_embed.proj.bias = old_patch_embed['proj.bias']
            self.patch_embed.proj.weight = old_patch_embed['proj.weight']

        # Pos embedding
        patch_pos_embed = self.pos_embed[:, patch_start_index:, :]
        patch_pos_embed = patch_pos_embed.transpose(1, 2) #?
        B, E, Q = patch_pos_embed.shape
        P_H, P_W = self.patch_embed.patch_shape[0], self.patch_embed.patch_shape[1]
        patch_pos_embed = patch_pos_embed.view(B, E, P_H, P_W)

        # Pos embedding for search region
        H, W = search_size
        new_P_H_S, new_P_W_S = H // new_patch_size, W // new_patch_size
        search_patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H_S, new_P_W_S), mode='bicubic',
                                                           align_corners=False)
        search_patch_pos_embed = search_patch_pos_embed.flatten(2).transpose(1, 2)

        # Pos embedding for template region
        H, W = template_size
        new_P_H_T, new_P_W_T = H // new_patch_size, W // new_patch_size
        template_patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H_T, new_P_W_T), mode='bicubic',
                                                             align_corners=False)
        template_patch_pos_embed = template_patch_pos_embed.flatten(2).transpose(1, 2)

        self.pos_embed_z = nn.Parameter(template_patch_pos_embed)
        self.pos_embed_x = nn.Parameter(search_patch_pos_embed)

        # for cls token 
        if self.add_cls_token and patch_start_index > 0:
            cls_pos_embed = self.pos_embed[:, 0:1, :]
            self.cls_pos_embed = nn.Parameter(cls_pos_embed)

        # separate token and segment token
        if self.add_sep_seg:
            self.template_segment_pos_embed = nn.Parameter(torch.zeros(1, 1, self.embed_dim))
            self.template_segment_pos_embed = trunc_normal_(self.template_segment_pos_embed, std=.02)
            self.search_segment_pos_embed = nn.Parameter(torch.zeros(1, 1, self.embed_dim))
            self.search_segment_pos_embed = trunc_normal_(self.search_segment_pos_embed, std=.02)

        # self.cls_token = None
        # self.pos_embed = None

        if self.return_inter:
            for i_layer in self.fpn_stage:
                if i_layer != 11:
                    norm_layer = partial(nn.LayerNorm, eps=1e-6)
                    layer = norm_layer(self.embed_dim)
                    layer_name = f'norm{i_layer}'
                    self.add_module(layer_name, layer)

        if self.add_template_target_embed:
            self.template_target_embed = PatchEmbed(
                img_size=template_size, patch_size=new_patch_size, in_chans=1)

        # For 8x8 tokens in Template and 16x16 tokens in Search, the size of attn map is 8x8+16x16 = 320
        attn_map_size = new_P_W_T * new_P_H_T + new_P_W_S * new_P_H_S   
        if self.add_cls_token:
            attn_map_size += 1
        center_attn_mask = torch.ones([attn_map_size, attn_map_size], dtype=torch.bool)
        # center_attn_mask[:new_P_W_T * new_P_H_T, :new_P_W_T * new_P_H_T] = True # Self attn in Template
        # center_attn_mask[new_P_W_T * new_P_H_T:, new_P_W_T * new_P_H_T:] = True # Self attn in Search
        # center_attn_mask[new_P_W_T * new_P_H_T//2] = True # Template to Search attn 
        # center_attn_mask[:, new_P_W_T * new_P_H_T//2] = True # Search to Template attn
        self.register_buffer('center_attn_mask', center_attn_mask)
   
    def train(self, mode=True):
        """Convert the model into training mode while keep layers freezed."""
        super(VIT_Backbone, self).train(mode)
        self._freeze_stages()
        # if self.pos_embed is not None:
        #     if self.pos_embed.requires_grad:
        #         print("=================pos_embed update ================")
        #     else:
        #         print("=================pos_embed static ================")
            
    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for param in self.patch_embed.parameters():
                param.requires_grad = False
            self.cls_token.requires_grad = False
            if self.pos_embed is not None and self.use_sincos_pos_emb == True:
                self.pos_embed.requires_grad = False
            self.pos_drop.eval()

        for i in range(1, self.frozen_stages + 1):
            
            if i  == len(self.blocks):
                norm_layer = getattr(self, 'norm') #f'norm{i-1}')
                norm_layer.eval()
                for param in norm_layer.parameters():
                    param.requires_grad = False

            m = self.blocks[i - 1]
            m.eval()
            for param in m.parameters():
                param.requires_grad = False
        
    def interpolate_pos_encoding(self, x, w, h):
        npatch = x.shape[1] - 1
        N = self.pos_embed.shape[1] - 1
        w0 = w // self.patch_embed.patch_size
        h0 = h // self.patch_embed.patch_size
        if npatch == N and w0 == self.patch_embed.num_patches_w and h0 == self.patch_embed.num_patches_h:
            return self.pos_embed
        class_pos_embed = self.pos_embed[:, 0]
        patch_pos_embed = self.pos_embed[:, 1:]
        dim = x.shape[-1]
        # we add a small number to avoid floating point error in the interpolation
        # see discussion at https://github.com/facebookresearch/dino/issues/8
        w0, h0 = w0 + 0.1, h0 + 0.1
        
        tmp=patch_pos_embed.reshape(1, self.patch_embed.num_patches_w, self.patch_embed.num_patches_h, dim).permute(0, 3, 1, 2)
        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed.reshape(1, self.patch_embed.num_patches_w, self.patch_embed.num_patches_h, dim).permute(0, 3, 1, 2),
            scale_factor=(w0 / self.patch_embed.num_patches_w, h0 / self.patch_embed.num_patches_h),
            mode='bicubic',
        )
        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        # return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)
        return patch_pos_embed

    def forward_features(self, z, x, mask_z=None, mask_x=None,
                         ce_template_mask=None, ce_keep_rate=None,
                         return_last_attn=False, template_bb=None,
                         target_token=None
                         ):
        B, H, W = x.shape[0], x.shape[2], x.shape[3]

        x = self.patch_embed(x)
        # z = self.patch_embed(z)
        if self.add_template_target_embed:
            target_embed = self.template_target_embed(mask_z.unsqueeze(1).float())
            z += target_embed

        # attention mask handling
        # B, H, W
        # if mask_z is not None and mask_x is not None:
        #     mask_z = F.interpolate(mask_z[None].float(), scale_factor=1. / self.patch_size).to(torch.bool)[0]
        #     mask_z = mask_z.flatten(1).unsqueeze(-1)

        #     mask_x = F.interpolate(mask_x[None].float(), scale_factor=1. / self.patch_size).to(torch.bool)[0]
        #     mask_x = mask_x.flatten(1).unsqueeze(-1)

        #     mask_x = combine_tokens(mask_z, mask_x, mode=self.cat_mode)
        #     mask_x = mask_x.squeeze(-1)

        # if self.add_cls_token:
        #     cls_tokens = self.cls_embed(template_bb).unsqueeze(-2)
            # cls_tokens = self.cls_token.expand(B, -1, -1)
            # cls_tokens = cls_tokens + self.cls_pos_embed

        # z += self.pos_embed_z
        x += self.pos_embed_x

        if self.add_sep_seg:
            x += self.search_segment_pos_embed
            z += self.template_segment_pos_embed

        if self.add_cls_token:
            z = torch.cat([target_token, z], dim=1)
        # x = combine_tokens(z, x, mode=self.cat_mode)       
        
        x = self.pos_drop(x)
        # z = self.pos_drop(z)

        lens_z = self.pos_embed_z.shape[1]
        lens_x = self.pos_embed_x.shape[1]

        # global_index_t = torch.linspace(0, lens_z - 1, lens_z).to(x.device)
        # global_index_t = global_index_t.repeat(B, 1)

        # global_index_s = torch.linspace(0, lens_x - 1, lens_x).to(x.device)
        # global_index_s = global_index_s.repeat(B, 1)
        removed_indexes_s = []
        attns = []
        if self.CE:
            removed_indexes_s = []
            for i, blk in enumerate(self.blocks):
                x, global_index_t, global_index_s, removed_index_s, attn = \
                    blk(x, global_index_t, global_index_s, x_rel_pos_bias=None,
                        mask=self.center_attn_mask, ce_template_mask=ce_template_mask,
                        keep_ratio_search=ce_keep_rate, use_target_token=self.add_cls_token)

                if self.ce_loc is not None and i in self.ce_loc:
                    removed_indexes_s.append(removed_index_s)
        else:
            # z = x[:, :lens_z].to(torch.float32)
            # x = x[:, lens_z:]

            # # Async:
            # for i in range(0,9):
            #     z, attn = self.blocks[i](z, 0, None,  mask=None)
            if self.add_cls_token:
                z = z[:,1:]
            for i in range(0,4):
                x, attn = self.blocks[i](x, 0, None,  mask=None)
            x = combine_tokens(z, x, mode=self.cat_mode)
            for i in range(4,6):
                x, attn = self.blocks[i](x, lens_z, None,  mask=None)
            
            # One-Stream:
            # for i in range(0,12):
            #     x, attn = self.blocks[i](x, 0, None,  mask=None)

            

            # x = torch.cat([z, x], dim=1)

            # # attentive_index = 0
            # # for i, blk in enumerate(self.blocks):
            # #     if i in [5, 7, 11]:
            # x = self.attentive_blocks[0](x,z,0,0, bool_masked_pos=None, rel_pos_bias=None)
            #         attentive_index += 1
            #     x, attn = blk(x, lens_z, None,  mask=self.center_attn_mask)
        x = self.norm(x)
        # lens_x_new = global_index_s.shape[1]
        # lens_z_new = global_index_t.shape[1]
        if self.add_cls_token:
            z = x[:, :lens_z]
            x = x[:, lens_z:]
        else:
            z = x[:, :lens_z]
            x = x[:, lens_z:]

        if removed_indexes_s and removed_indexes_s[0] is not None:
            removed_indexes_cat = torch.cat(removed_indexes_s, dim=1)

            pruned_lens_x = lens_x - lens_x_new
            pad_x = torch.zeros([B, pruned_lens_x, x.shape[2]], device=x.device)
            x = torch.cat([x, pad_x], dim=1)
            index_all = torch.cat([global_index_s, removed_indexes_cat], dim=1)
            # recover original token order
            C = x.shape[-1]
            # x = x.gather(1, index_all.unsqueeze(-1).expand(B, -1, C).argsort(1))
            x = torch.zeros_like(x).scatter_(dim=1, index=index_all.unsqueeze(-1).expand(B, -1, C).to(torch.int64), src=x)

        # x = recover_tokens(x, lens_z_new, lens_x, mode=self.cat_mode)

        # re-concatenate with the template, which may be further used by other modules
        x = torch.cat([z, x], dim=1)
        # attns = torch.cat(attns, dim=1)
        aux_dict = {
            # "attn": attns[:,:,lens_z:,:lens_z].mean(-1).reshape(B,-1,16,16),
            "attn": attn,
            "removed_indexes_s": removed_indexes_s,  # used for visualization
        }

        return x, aux_dict

    def forward(self, z, x, ce_template_mask=None, ce_keep_rate=None,
                tnc_keep_rate=None,
                return_last_attn=False,
                template_bb=None,
                mask_z=None,
                target_token=None):

        x, aux_dict = self.forward_features(z, x, ce_template_mask=ce_template_mask,
                ce_keep_rate=ce_keep_rate,template_bb=template_bb, mask_z=mask_z, target_token=target_token)

        return x, aux_dict



def _create_ViT_CAE(pretrained=False, **kwargs) -> VIT_Backbone: 
    model = VIT_Backbone(**kwargs)

    if pretrained:
        if 'npz' in pretrained:
            model.load_pretrained(pretrained, prefix='')
        else:
            state_dict = torch.load('pretrained_models/cae_base.pth', map_location="cpu")
            # new_ckpt = OrderedDict()
            state_dict = state_dict['model']
            if sorted(list(state_dict.keys()))[0].startswith('encoder'):
                state_dict = {k.replace('encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}
            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
            print("Unexpected keys:")
            print(unexpected_keys)
            print("Missing keys:")
            print(missing_keys)
    return model

def CAE_Base_patch16_224(pretrained=False, **kwargs) -> VIT_Backbone:
    model_kwargs = dict(mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),
        patch_size=16, embed_dim=768, depth=12, num_heads=12,
        drop_rate=0., attn_drop_rate=0., init_values=0.1, 
        use_abs_pos_emb=True, **kwargs)
    model = _create_ViT_CAE(pretrained=pretrained, **model_kwargs)
    # model.adpat4track()
    return model