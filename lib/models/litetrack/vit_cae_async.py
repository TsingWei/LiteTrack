# Copyright (c) ByteDance, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
Mostly copy-paste from DINO and timm library:
https://github.com/facebookresearch/dino
https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
"""


import math
import torch
import torch.nn as nn
import torch.nn.functional as F


from functools import partial
from timm.models.layers import to_2tuple


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    # if (mean < a - 2 * std) or (mean > b + 2 * std):
    #     warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
    #                   "The distribution of values may be incorrect.",
    #                   stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    # work with diff dim tensors, not just 2D ConvNets
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = keep_prob + \
        torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=None):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        # add by wxd
        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        all_head_dim = head_dim * self.num_heads
        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))
            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))
        else:
            self.q_bias = None
            self.v_bias = None
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, len_t=None, x_rel_pos_bias=None, return_attention=True, mask=None):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(
                self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        if x_rel_pos_bias is not None:
            attn = attn + x_rel_pos_bias.unsqueeze(0)
        if mask is not None:
            attn = attn.masked_fill(~mask, float('-inf'),)
        attn_nosf = attn
        attn = attn.softmax(dim=-1)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        if return_attention:
            if mask is not None:
                attn = attn[:, :, :-1, :-1]
            return x, attn_nosf
        else:
            return x


class MixedCrossAttention(Attention):
    def forward(self, x, len_t):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(
                self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        q_s = q[:, :, len_t:]

        attn = (q_s @ k.transpose(-2, -1)) * self.scale

        attn = self.attn_drop(attn)

        attn_s2ts = attn
        attn_s2ts = attn_s2ts.softmax(dim=-1)

        x_t = x[:, :len_t]
        x_s = (attn_s2ts @ v).transpose(1, 2).reshape(B, N-len_t, C)
        x = torch.cat((x_t, x_s), dim=1)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., seperate_self_attn=False,
                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, window_size=None, init_values=0,
                 add_cls_token=False):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.add_cls_token = add_cls_token
        self.drop_path = DropPath(
            drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=act_layer, drop=drop)
        self.attn = MixedCrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
                                        attn_drop=attn_drop, proj_drop=drop, window_size=window_size)

        if init_values > 0:
            self.gamma_1 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True)
            self.gamma_2 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True)
        else:
            self.gamma_1, self.gamma_2 = None, None

    def forward_old(self, x, len_t):
        y = self.attn(self.norm1(x), len_t)
        if self.gamma_1 is None:
            x = x + self.drop_path(y)
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        else:
            x = x + self.drop_path(self.gamma_1 * y)
            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
        return x

    def forward(self, x, len_t):
        # Here we keep the template part unchanged
        x_t = x[:, :len_t]
        x_s = x[:, len_t:]

        y= self.attn(self.norm1(
            torch.cat((x_t, x_s), dim=1)), len_t)
        y_s = y[:, len_t:]

        if self.gamma_1 is None:
            x = x + self.drop_path(y)
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        else:
            x_s = x_s + self.drop_path(self.gamma_1 * y_s)
            x_s = x_s + self.drop_path(self.gamma_2 *
                                       self.mlp(self.norm2(x_s)))
            x = torch.cat((x_t, x_s), dim=1)
        return x


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """

    def __init__(self, img_size=[224, 224], patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.num_patches_w = img_size[0] // patch_size
        self.num_patches_h = img_size[1] // patch_size

        num_patches = self.num_patches_w * self.num_patches_h
        self.patch_shape = (img_size[0] // patch_size,
                            img_size[1] // patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        # print("##############patch here!!!")
        self.proj = nn.Conv2d(in_chans, embed_dim,
                              kernel_size=patch_size, stride=patch_size)
        self.flatten = True

    def forward(self, x, mask=None):
        # B, C, H, W = x.shape
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
        # x = self.norm(x)
        return x


class VIT_Backbone(nn.Module):
    def __init__(self,
                 img_size=[224, 224],
                 patch_size=16,
                 embed_dim=768,
                 in_chans=3,
                 frozen_stages=-1,
                 #  out_indices=[3, 5, 7, 11],
                 out_with_norm=False,
                 use_checkpoint=False,
                 num_async_interaction_stage=3,
                 add_cls_token=False,
                 add_target_token=True,

                 num_classes=0,
                 depth=12,
                 num_heads=12,
                 mlp_ratio=4.,
                 qkv_bias=False,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.,
                 norm_layer=partial(nn.LayerNorm, eps=1e-6),
                 return_all_tokens=False,
                 init_values=0,
                 use_sincos_pos_emb=False,
                 use_abs_pos_emb=False,
                 use_rel_pos_bias=False,
                 seperate_loc=None,
                 act_layer=None,
                 ce_loc=None,
                 ce_keep_ratio=None,
                 use_mean_pooling=False,
                 masked_im_modeling=False,
                 ):

        super().__init__()

        self.num_features = self.embed_dim = embed_dim
        self.return_all_tokens = return_all_tokens
        self.use_abs_pos_emb = use_abs_pos_emb
        self.use_sincos_pos_emb = use_sincos_pos_emb
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        if use_abs_pos_emb:
            self.pos_embed = nn.Parameter(
                torch.zeros(1, num_patches + 1, embed_dim))
        else:
            self.pos_embed = None

        self.pos_drop = nn.Dropout(p=drop_rate)
        act_layer = act_layer or nn.GELU
        self.use_rel_pos_bias = use_rel_pos_bias

        if self.use_rel_pos_bias:
            print("=================use RelativePositionBias===================")
            window_size = self.patch_embed.patch_shape
            self.window_size = window_size
            self.num_relative_distance = (
                2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3
            self.relative_position_bias_table = nn.Parameter(
                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH
            # cls to token & token 2 cls & cls to cls

        else:
            self.window_size = None
            self.relative_position_bias_table = None
            self.relative_position_index = None

        # stochastic depth decay rule
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]

        blocks = []

        for i in range(depth):
            blocks.append(Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[
                    i], norm_layer=norm_layer,
                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None))
        self.blocks = nn.ModuleList(blocks)

        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)
        if use_abs_pos_emb:
            trunc_normal_(self.pos_embed, std=.02)
        trunc_normal_(self.cls_token, std=.02)
        self.apply(self._init_weights)

        # masked image modeling
        self.masked_im_modeling = masked_im_modeling
        if masked_im_modeling:
            self.masked_embed = nn.Parameter(torch.zeros(1, embed_dim))

        # support non-square image as input
        if len(img_size) == 1:
            img_size = img_size * 2
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches
        if self.use_abs_pos_emb:
            self.pos_embed = nn.Parameter(
                torch.zeros(1, num_patches + 1, embed_dim))
        elif self.use_sincos_pos_emb:
            self.pos_embed = self.build_2d_sincos_position_embedding(embed_dim)
        else:
            self.pos_embed = None

        self.add_target_token = add_target_token
        self.patch_size = patch_size
        self.frozen_stages = frozen_stages
        self.use_checkpoint = use_checkpoint
        self.num_FE_layers = depth - num_async_interaction_stage
        self.depth = depth
        # self.num_async_interaction_stage = num_async_interaction_stage

        self.pos_embed_z = None
        self.pos_embed_x = None

        if not out_with_norm:
            self.norm = nn.Identity()

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def finetune_track(self, cfg, patch_start_index=1):
        # patch_start_index: For skipping the cls embedding
        search_size = to_2tuple(cfg.DATA.SEARCH.SIZE)
        template_size = to_2tuple(cfg.DATA.TEMPLATE.SIZE)
        new_patch_size = cfg.MODEL.BACKBONE.STRIDE

        # resize patch embedding
        if new_patch_size != self.patch_size:
            print(
                'Inconsistent Patch Size With The Pretrained Weights, Interpolate The Weight!')
            old_patch_embed = {}
            for name, param in self.patch_embed.named_parameters():
                if 'weight' in name:
                    param = nn.functional.interpolate(param, size=(new_patch_size, new_patch_size),
                                                      mode='bicubic', align_corners=False)
                    param = nn.Parameter(param)
                old_patch_embed[name] = param
            # self.patch_embed = PatchEmbed(img_size=self.img_size, patch_size=new_patch_size, in_chans=3,
            #                               embed_dim=self.embed_dim)
            self.patch_embed.proj.bias = old_patch_embed['proj.bias']
            self.patch_embed.proj.weight = old_patch_embed['proj.weight']

        # Pos embedding
        patch_pos_embed = self.pos_embed[:, patch_start_index:, :]
        patch_pos_embed = patch_pos_embed.transpose(1, 2)  # ?
        B, E, Q = patch_pos_embed.shape
        P_H, P_W = self.patch_embed.patch_shape[0], self.patch_embed.patch_shape[1]
        patch_pos_embed = patch_pos_embed.view(B, E, P_H, P_W)

        # Pos embedding for search region
        H, W = search_size
        new_P_H_S, new_P_W_S = H // new_patch_size, W // new_patch_size
        search_patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H_S, new_P_W_S), mode='bicubic',
                                                           align_corners=False)
        search_patch_pos_embed = search_patch_pos_embed.flatten(
            2).transpose(1, 2)

        # Pos embedding for template region
        H, W = template_size
        new_P_H_T, new_P_W_T = H // new_patch_size, W // new_patch_size
        template_patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H_T, new_P_W_T), mode='bicubic',
                                                             align_corners=False)
        template_patch_pos_embed = template_patch_pos_embed.flatten(
            2).transpose(1, 2)

        self.pos_embed_z = nn.Parameter(template_patch_pos_embed)
        self.pos_embed_x = nn.Parameter(search_patch_pos_embed)



    def train(self, mode=True):
        """Convert the model into training mode while keep layers freezed."""
        super(VIT_Backbone, self).train(mode)
        self._freeze_stages()

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for param in self.patch_embed.parameters():
                param.requires_grad = False
            self.cls_token.requires_grad = False
            if self.pos_embed is not None and self.use_sincos_pos_emb == True:
                self.pos_embed.requires_grad = False
            self.pos_drop.eval()

        for i in range(1, self.frozen_stages + 1):

            if i == len(self.blocks):
                norm_layer = getattr(self, 'norm')  # f'norm{i-1}')
                norm_layer.eval()
                for param in norm_layer.parameters():
                    param.requires_grad = False

            m = self.blocks[i - 1]
            m.eval()
            for param in m.parameters():
                param.requires_grad = False

    def forward_zx(self, z, x, target_token=None):
        x = self.patch_embed(x)
        x += self.pos_embed_x
        x = self.pos_drop(x)

        len_z = self.pos_embed_z.shape[1]

        if self.add_target_token and target_token is not None:
            z = torch.cat([target_token, z], dim=1)

        for i in range(self.num_FE_layers):
            x = self.blocks[i](x, 0)

        if self.add_target_token and target_token is not None:
            z = z[:,1:]

        x = torch.cat((z, x),dim=1)
        for i in range(self.num_FE_layers, self.depth):
            x = self.blocks[i](x, len_z)

        x = self.norm(x)
        return x

    def forward_z(self,
                  z,
                  target_token=None,
                  ):

        z = self.patch_embed(z)
        z += self.pos_embed_z
        z = self.pos_drop(z)

        if self.add_target_token and target_token is not None:
            z = torch.cat([target_token, z], dim=1)

        for i, blk in enumerate(self.blocks):
            z = blk(z, 0)

        if self.add_target_token and target_token is not None:
            z = z[:,1:]

        return z

    def forward(self, z, x=None,
                target_token=None,
                mode=None
                ):
        if mode is None:
            z_feat = self.forward_z(z)
            x_feat = self.forward_zx(z_feat, x, target_token=target_token)
        elif mode == 'x':
            x_feat = self.forward_zx(z, x)
        elif mode == 'z':
            x_feat = self.forward_z(z, target_token=target_token)


        return x_feat


def _create_ViT_CAE(pretrained=False, **kwargs) -> VIT_Backbone:
    model = VIT_Backbone(**kwargs)

    if pretrained:
        if 'npz' in pretrained:
            model.load_pretrained(pretrained, prefix='')
        else:
            state_dict = torch.load(
                'pretrained_models/cae_base.pth', map_location="cpu")
            # new_ckpt = OrderedDict()
            state_dict = state_dict['model']
            if sorted(list(state_dict.keys()))[0].startswith('encoder'):
                state_dict = {k.replace(
                    'encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}
            missing_keys, unexpected_keys = model.load_state_dict(
                state_dict, strict=False)
            print("Unexpected keys:")
            print(unexpected_keys)
            print("Missing keys:")
            print(missing_keys)
    return model


def CAE_Base_patch16_224_Async(pretrained=False, **kwargs) -> VIT_Backbone:
    model_kwargs = dict(mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),
                        patch_size=16, embed_dim=768, num_heads=12,
                        drop_rate=0., attn_drop_rate=0., init_values=0.1,
                        use_abs_pos_emb=True, **kwargs)
    model = _create_ViT_CAE(pretrained=pretrained, **model_kwargs)
    # model.adpat4track()
    return model
